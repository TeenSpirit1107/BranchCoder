{"docstore/metadata": {"file::0": {"doc_hash": "2f4ea4ac5f041d7b141e9e31fa60a3b2c96c80ebcc41ac0ed6a3ebdc3babdccb"}, "file::1": {"doc_hash": "b6a4434b0a77d0437e9d13dff4be3d77f76c9b7882cd020cce235b8923d0f5e2"}, "file::2": {"doc_hash": "b70dcb164797fb875aaf4483f241d0a326654348f22e74d5784c1c8a65cf43c6"}, "3af61f7a-fc95-43e0-ba8e-3d415f7cbb63": {"doc_hash": "9f4888791b9da94c4f990f49df0e70d9692221d5740b6a920cc99f55f9c4f596", "ref_doc_id": "file::0"}, "e29aaab4-6e88-474d-bf48-a5b7a851f657": {"doc_hash": "ed37f945c2c32eacab5cada96e9cda05264bbc2e9066abbee57d336dd4f83275", "ref_doc_id": "file::1"}, "64abe6c4-14a7-438f-9381-3c7a6e96511c": {"doc_hash": "6cdb4c9ef018cfbfabf0cb0fe11cb4845bcbe2946b6d033b1668e20fb59681a3", "ref_doc_id": "file::2"}}, "docstore/ref_doc_info": {"file::0": {"node_ids": ["3af61f7a-fc95-43e0-ba8e-3d415f7cbb63"], "metadata": {"type": "file", "idx": 0, "file": "softmax_regression.py"}}, "file::1": {"node_ids": ["e29aaab4-6e88-474d-bf48-a5b7a851f657"], "metadata": {"type": "file", "idx": 1, "file": "_base_network.py"}}, "file::2": {"node_ids": ["64abe6c4-14a7-438f-9381-3c7a6e96511c"], "metadata": {"type": "file", "idx": 2, "file": "two_layer_nn.py"}}}, "docstore/data": {"3af61f7a-fc95-43e0-ba8e-3d415f7cbb63": {"__data__": {"id_": "3af61f7a-fc95-43e0-ba8e-3d415f7cbb63", "embedding": null, "metadata": {"type": "file", "idx": 0, "file": "softmax_regression.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::0", "node_type": "4", "metadata": {"type": "file", "idx": 0, "file": "softmax_regression.py"}, "hash": "2f4ea4ac5f041d7b141e9e31fa60a3b2c96c80ebcc41ac0ed6a3ebdc3babdccb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "SoftmaxRegression: Defines a single-layer softmax regression classifier with no bias. It inherits from _baseNetwork to reuse common utilities for activations, loss calculation, accuracy, and gradient storage. The class initializes a weight matrix W1 and corresponding gradient buffers, seeding NumPy for reproducibility. The forward method supports both training and evaluation: it computes a linear projection, applies ReLU, then softmax, and derives cross-entropy loss and accuracy; in training mode it also backpropagates to compute and store gradients w.r.t. W1. It relies on NumPy and the base network module in the same package for core operations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 654, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e29aaab4-6e88-474d-bf48-a5b7a851f657": {"__data__": {"id_": "e29aaab4-6e88-474d-bf48-a5b7a851f657", "embedding": null, "metadata": {"type": "file", "idx": 1, "file": "_base_network.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::1", "node_type": "4", "metadata": {"type": "file", "idx": 1, "file": "_base_network.py"}, "hash": "b6a4434b0a77d0437e9d13dff4be3d77f76c9b7882cd020cce235b8923d0f5e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "File-level summary This module defines a base neural network class using NumPy, intended to be extended by concrete models. It stores weights and gradients in dictionaries and provides a set of common utilities for neural networks, including softmax, cross-entropy loss, accuracy computation, and basic activation functions (sigmoid and ReLU) with their derivatives. The forward pass and weight initialization are left as placeholders for subclasses to implement. The implementation is self-contained, with no external dependencies beyond NumPy, and emphasizes numerical stability (e.g., softmax) and safe loss computation. The class interacts with training loops and layer-specific components via standardized interfaces and shared data structures (weights and gradients).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 773, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "64abe6c4-14a7-438f-9381-3c7a6e96511c": {"__data__": {"id_": "64abe6c4-14a7-438f-9381-3c7a6e96511c", "embedding": null, "metadata": {"type": "file", "idx": 2, "file": "two_layer_nn.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::2", "node_type": "4", "metadata": {"type": "file", "idx": 2, "file": "two_layer_nn.py"}, "hash": "b70dcb164797fb875aaf4483f241d0a326654348f22e74d5784c1c8a65cf43c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "TwoLayerNet module implementing a simple two-layer neural network for classification. It extends a base network (_baseNetwork) to reuse utilities for activations, loss calculation, and accuracy metrics. The file handles deterministic weight initialization, forward computation (including loss, accuracy, and gradient generation), and backpropagation to populate gradients for W1, b1, W2, and b2. It relies on numpy for numeric operations and on methods provided by the base class such as sigmoid, sigmoid_dev, softmax, cross_entropy_loss, and compute_accuracy. This structure is designed to be used by training loops that update parameters via the stored gradients.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 665, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}