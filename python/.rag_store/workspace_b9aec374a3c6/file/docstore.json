{"docstore/metadata": {"file::0": {"doc_hash": "8196f6e074f15c383c3c3635bf6d219a25b50054d27dc2f8b404e47dc0ff910f"}, "file::1": {"doc_hash": "b6a4434b0a77d0437e9d13dff4be3d77f76c9b7882cd020cce235b8923d0f5e2"}, "file::2": {"doc_hash": "b70dcb164797fb875aaf4483f241d0a326654348f22e74d5784c1c8a65cf43c6"}, "3af61f7a-fc95-43e0-ba8e-3d415f7cbb63": {"doc_hash": "9f4888791b9da94c4f990f49df0e70d9692221d5740b6a920cc99f55f9c4f596", "ref_doc_id": "file::0"}, "e29aaab4-6e88-474d-bf48-a5b7a851f657": {"doc_hash": "ed37f945c2c32eacab5cada96e9cda05264bbc2e9066abbee57d336dd4f83275", "ref_doc_id": "file::1"}, "64abe6c4-14a7-438f-9381-3c7a6e96511c": {"doc_hash": "6cdb4c9ef018cfbfabf0cb0fe11cb4845bcbe2946b6d033b1668e20fb59681a3", "ref_doc_id": "file::2"}, "a82d70f4-16bf-4335-882a-221c10b8c9cb": {"doc_hash": "3fc221516d6a74119a11e1746e3e991606ffc3a5a588707a502272377927ae08", "ref_doc_id": "file::0"}, "b02d1fd5-36f7-42b8-9738-7da52d0650d6": {"doc_hash": "e59eb52f2a96a765afafd605f21ff805a99345387128c86074dcdcaeb932021b", "ref_doc_id": "file::0"}, "468a3a23-afec-43b9-960e-3fd1b6ff4b13": {"doc_hash": "0d4018486a5f7f6b942960556fc78b5f9ba014291eb51aedaeb3fa4f5b0837f9", "ref_doc_id": "file::0"}, "120bb69f-6d57-4e96-b192-221b87d55b5e": {"doc_hash": "58d46aae3b63a21aec2c17db47a2f1c417274f0380cf51243e2643081519b4ae", "ref_doc_id": "file::0"}}, "docstore/ref_doc_info": {"file::0": {"node_ids": ["3af61f7a-fc95-43e0-ba8e-3d415f7cbb63", "a82d70f4-16bf-4335-882a-221c10b8c9cb", "b02d1fd5-36f7-42b8-9738-7da52d0650d6", "468a3a23-afec-43b9-960e-3fd1b6ff4b13", "120bb69f-6d57-4e96-b192-221b87d55b5e"], "metadata": {"type": "file", "idx": 0, "file": "softmax_regression.py"}}, "file::1": {"node_ids": ["e29aaab4-6e88-474d-bf48-a5b7a851f657"], "metadata": {"type": "file", "idx": 1, "file": "_base_network.py"}}, "file::2": {"node_ids": ["64abe6c4-14a7-438f-9381-3c7a6e96511c"], "metadata": {"type": "file", "idx": 2, "file": "two_layer_nn.py"}}}, "docstore/data": {"3af61f7a-fc95-43e0-ba8e-3d415f7cbb63": {"__data__": {"id_": "3af61f7a-fc95-43e0-ba8e-3d415f7cbb63", "embedding": null, "metadata": {"type": "file", "idx": 0, "file": "softmax_regression.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::0", "node_type": "4", "metadata": {"type": "file", "idx": 0, "file": "softmax_regression.py"}, "hash": "2f4ea4ac5f041d7b141e9e31fa60a3b2c96c80ebcc41ac0ed6a3ebdc3babdccb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "SoftmaxRegression: Defines a single-layer softmax regression classifier with no bias. It inherits from _baseNetwork to reuse common utilities for activations, loss calculation, accuracy, and gradient storage. The class initializes a weight matrix W1 and corresponding gradient buffers, seeding NumPy for reproducibility. The forward method supports both training and evaluation: it computes a linear projection, applies ReLU, then softmax, and derives cross-entropy loss and accuracy; in training mode it also backpropagates to compute and store gradients w.r.t. W1. It relies on NumPy and the base network module in the same package for core operations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 654, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "e29aaab4-6e88-474d-bf48-a5b7a851f657": {"__data__": {"id_": "e29aaab4-6e88-474d-bf48-a5b7a851f657", "embedding": null, "metadata": {"type": "file", "idx": 1, "file": "_base_network.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::1", "node_type": "4", "metadata": {"type": "file", "idx": 1, "file": "_base_network.py"}, "hash": "b6a4434b0a77d0437e9d13dff4be3d77f76c9b7882cd020cce235b8923d0f5e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "File-level summary This module defines a base neural network class using NumPy, intended to be extended by concrete models. It stores weights and gradients in dictionaries and provides a set of common utilities for neural networks, including softmax, cross-entropy loss, accuracy computation, and basic activation functions (sigmoid and ReLU) with their derivatives. The forward pass and weight initialization are left as placeholders for subclasses to implement. The implementation is self-contained, with no external dependencies beyond NumPy, and emphasizes numerical stability (e.g., softmax) and safe loss computation. The class interacts with training loops and layer-specific components via standardized interfaces and shared data structures (weights and gradients).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 773, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "64abe6c4-14a7-438f-9381-3c7a6e96511c": {"__data__": {"id_": "64abe6c4-14a7-438f-9381-3c7a6e96511c", "embedding": null, "metadata": {"type": "file", "idx": 2, "file": "two_layer_nn.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::2", "node_type": "4", "metadata": {"type": "file", "idx": 2, "file": "two_layer_nn.py"}, "hash": "b70dcb164797fb875aaf4483f241d0a326654348f22e74d5784c1c8a65cf43c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "TwoLayerNet module implementing a simple two-layer neural network for classification. It extends a base network (_baseNetwork) to reuse utilities for activations, loss calculation, and accuracy metrics. The file handles deterministic weight initialization, forward computation (including loss, accuracy, and gradient generation), and backpropagation to populate gradients for W1, b1, W2, and b2. It relies on numpy for numeric operations and on methods provided by the base class such as sigmoid, sigmoid_dev, softmax, cross_entropy_loss, and compute_accuracy. This structure is designed to be used by training loops that update parameters via the stored gradients.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 665, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a82d70f4-16bf-4335-882a-221c10b8c9cb": {"__data__": {"id_": "a82d70f4-16bf-4335-882a-221c10b8c9cb", "embedding": null, "metadata": {"type": "file", "idx": 0, "file": "_base_network.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::0", "node_type": "4", "metadata": {"type": "file", "idx": 0, "file": "_base_network.py"}, "hash": "a81757543e92ef44c40ae4afbb53307cd73f07b2660c6e9ced137a48cfa70182", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A minimal base neural network module intended to be subclassed by concrete models. It defines configuration attributes like input_size and num_classes, and stores weights and gradients in dictionaries for easy extension. It provides common utility functions used in neural nets, including a numerically stable softmax, cross-entropy loss, and accuracy calculation, as well as basic activations (sigmoid and ReLU) with their derivatives. The file includes placeholder methods for weight initialization and forward pass to be implemented by subclasses. A small top-level greeting function is included to facilitate imports in tests. The module depends on NumPy and is designed to be extended by specific network implementations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 726, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b02d1fd5-36f7-42b8-9738-7da52d0650d6": {"__data__": {"id_": "b02d1fd5-36f7-42b8-9738-7da52d0650d6", "embedding": null, "metadata": {"type": "file", "idx": 0, "file": "_base_network.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::0", "node_type": "4", "metadata": {"type": "file", "idx": 0, "file": "_base_network.py"}, "hash": "0c2fef7a8ab4c6a29dbb5f15525f9fdc6fd358206d9626e0b7f62ba0e3053a21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The module defines a minimal base neural network class intended to be extended by concrete models. It stores core configuration like input_size and num_classes, and maintains dictionaries for weights and gradients. It provides utility implementations for common operations such as softmax, cross-entropy loss, accuracy measurement, and basic activation functions (sigmoid and ReLU) along with their derivatives. There are placeholder methods for weight initialization and the forward pass, signaling where subclasses should implement model-specific behavior. The code relies on NumPy and includes a small greeting helper for testing.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 633, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "468a3a23-afec-43b9-960e-3fd1b6ff4b13": {"__data__": {"id_": "468a3a23-afec-43b9-960e-3fd1b6ff4b13", "embedding": null, "metadata": {"type": "file", "idx": 0, "file": "_base_network.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::0", "node_type": "4", "metadata": {"type": "file", "idx": 0, "file": "_base_network.py"}, "hash": "18ccd29b09b34a764cc8e4632423d9e10b5ab80fb52e999cb1610b95e6990ef0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The file defines a base neural network class, _baseNetwork, intended to be extended by specific architectures. It uses NumPy for numerical operations and maintains dictionaries for weights and gradients. It provides common utility methods such as softmax, cross-entropy loss, accuracy computation, and common activation functions (sigmoid and ReLU) along with their derivatives. Some core methods like weight initialization and the forward pass are left as placeholders for subclasses to implement. The class tracks input size and number of classes, and includes a simple greeting helper for testing.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 600, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "120bb69f-6d57-4e96-b192-221b87d55b5e": {"__data__": {"id_": "120bb69f-6d57-4e96-b192-221b87d55b5e", "embedding": null, "metadata": {"type": "file", "idx": 0, "file": "_base_network.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::0", "node_type": "4", "metadata": {"type": "file", "idx": 0, "file": "_base_network.py"}, "hash": "8196f6e074f15c383c3c3635bf6d219a25b50054d27dc2f8b404e47dc0ff910f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The _base_network.py module defines a minimal base neural network class, primarily serving as a collection of common utilities and a lightweight scaffolding for more specialized networks. It imports numpy and exposes configuration (input_size, num_classes) along with dictionaries to hold weights and gradients. The class provides implementations for several standard activations (sigmoid, ReLU and their derivatives) and evaluation utilities (softmax, cross-entropy loss, accuracy) that can be reused by subclasses. Two placeholder methods (_weight_init and forward) are included for subclasses to implement concrete weight initialization and forward-pass logic. The module is designed to be extended by concrete network implementations and to support training workflows via numpy-based computations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 801, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}