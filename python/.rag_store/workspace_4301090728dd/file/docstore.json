{"docstore/metadata": {"file::0": {"doc_hash": "b746a8a99095096fabb2da679632a903977a335bbc63c5ce5e12b3d5e96e317d"}, "file::1": {"doc_hash": "b920498685bacb3057f94bdfe6ba6b864fa8dcc9a1c773f1a03a2c3763f65a2c"}, "file::2": {"doc_hash": "9cf63eebfe94a1aa88ac17005d8b11fa7846b49b5c23acf54d70a804e1a987e2"}, "8844e4ec-cfb8-402e-9c16-a16f13c4fdc2": {"doc_hash": "6b4e53e820185751d5e746048c200265dde89e6c42b6fdfa8efefa17f7b8b663", "ref_doc_id": "file::0"}, "a68dab52-e22e-47c9-887d-3dfabeefa40b": {"doc_hash": "77b96147cea815f6a127de64a3877d13b385e06d72c2ce0e40d86d13807e9121", "ref_doc_id": "file::1"}, "f0ef09b1-b588-4d00-b403-45aac717027e": {"doc_hash": "b72c6d8bbc3f3b73775981c231ab7aaee7ce8b08e25e7b473dbda5107137bf86", "ref_doc_id": "file::2"}}, "docstore/ref_doc_info": {"file::0": {"node_ids": ["8844e4ec-cfb8-402e-9c16-a16f13c4fdc2"], "metadata": {"type": "file", "idx": 0, "file": "softmax_regression.py"}}, "file::1": {"node_ids": ["a68dab52-e22e-47c9-887d-3dfabeefa40b"], "metadata": {"type": "file", "idx": 1, "file": "_base_network.py"}}, "file::2": {"node_ids": ["f0ef09b1-b588-4d00-b403-45aac717027e"], "metadata": {"type": "file", "idx": 2, "file": "two_layer_nn.py"}}}, "docstore/data": {"8844e4ec-cfb8-402e-9c16-a16f13c4fdc2": {"__data__": {"id_": "8844e4ec-cfb8-402e-9c16-a16f13c4fdc2", "embedding": null, "metadata": {"type": "file", "idx": 0, "file": "softmax_regression.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::0", "node_type": "4", "metadata": {"type": "file", "idx": 0, "file": "softmax_regression.py"}, "hash": "b746a8a99095096fabb2da679632a903977a335bbc63c5ce5e12b3d5e96e317d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "SoftmaxRegression implements a single-layer softmax classifier without a bias term. It performs a linear projection from input features to class scores, applies an optional ReLU activation, and then applies softmax to produce class probabilities. The forward method computes cross-entropy loss and accuracy, and during training it also computes gradients with respect to the weight matrix W1 and stores them in a gradients dictionary. The implementation relies on numpy and on a base class _baseNetwork for common utilities like ReLU, softmax, loss, and accuracy computations. It stores parameters and gradients in dictionaries (self.weights and self.gradients) and exposes a small API via __init__, _weight_init, and forward.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 726, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a68dab52-e22e-47c9-887d-3dfabeefa40b": {"__data__": {"id_": "a68dab52-e22e-47c9-887d-3dfabeefa40b", "embedding": null, "metadata": {"type": "file", "idx": 1, "file": "_base_network.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::1", "node_type": "4", "metadata": {"type": "file", "idx": 1, "file": "_base_network.py"}, "hash": "b920498685bacb3057f94bdfe6ba6b864fa8dcc9a1c773f1a03a2c3763f65a2c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[File-level summary] This module defines a foundational neural network class, _baseNetwork, intended for classification tasks (default input_size 28*28 and num_classes 10). It maintains dictionaries for weights and gradients to support parameterized models, and provides several utility methods for activations, loss computation, and accuracy assessment. The file includes implementations for softmax, cross-entropy loss, and common activations (sigmoid and ReLU) along with their derivatives. The actual weight initialization and forward-pass logic are left as placeholders to be implemented by subclasses. It relies solely on numpy and is designed to be extended by concrete network architectures.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 699, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f0ef09b1-b588-4d00-b403-45aac717027e": {"__data__": {"id_": "f0ef09b1-b588-4d00-b403-45aac717027e", "embedding": null, "metadata": {"type": "file", "idx": 2, "file": "two_layer_nn.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::2", "node_type": "4", "metadata": {"type": "file", "idx": 2, "file": "two_layer_nn.py"}, "hash": "9cf63eebfe94a1aa88ac17005d8b11fa7846b49b5c23acf54d70a804e1a987e2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "TwoLayerNet in two_layer_nn.py defines a compact 2-layer neural network with one hidden layer. It extends a base network to reuse utilities for activations, loss, and metrics, and to manage shared data structures like weights and gradients. The class initializes small random weights and zero biases, along with zero-initialized gradient buffers, via a dedicated weight initialization method. The forward method performs a two-stage forward pass (sigmoid activation between layers, softmax at the output), computes cross-entropy loss and accuracy, and computes/stores gradients for all parameters in the object. It relies on base-class utilities (sigmoid, sigmoid_dev, softmax, cross_entropy_loss, compute_accuracy) and numpy for numerical operations, and uses fixed seeds for reproducibility.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}