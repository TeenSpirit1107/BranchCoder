{"docstore/metadata": {"file::0": {"doc_hash": "90ef7986db952af8fc2516921e592c1525d75139a61a6b748bf513489010d97e"}, "file::1": {"doc_hash": "34836c22706b80c6a1037482a2ec15fa3b982751cf517c167080eb6c304bf977"}, "file::2": {"doc_hash": "709c1239821e2d515fccc767047984fbc776bd82d0b5ae26d6495da1fb072ea8"}, "dcc1e679-ff3c-4681-8bd8-a4792660a0d9": {"doc_hash": "cb89e5eee862309461732d5afe4111793c03c941a7368554b20655f4c5af86ab", "ref_doc_id": "file::0"}, "2a62e25b-1e6e-4c7a-8fce-1c92ea5347fe": {"doc_hash": "e834642a451fb1a0607f6a02e89006bdceb37caa7abcdd7b135f78c9639332a5", "ref_doc_id": "file::1"}, "1ebd511d-c00e-42fc-b6fb-941af1d1cd80": {"doc_hash": "89976ec09b62eb26611b41c331cb7bec49edd75d2f86c4159ed41d322f2421a7", "ref_doc_id": "file::2"}}, "docstore/ref_doc_info": {"file::0": {"node_ids": ["dcc1e679-ff3c-4681-8bd8-a4792660a0d9"], "metadata": {"type": "file", "idx": 0, "file": "softmax_regression.py"}}, "file::1": {"node_ids": ["2a62e25b-1e6e-4c7a-8fce-1c92ea5347fe"], "metadata": {"type": "file", "idx": 1, "file": "_base_network.py"}}, "file::2": {"node_ids": ["1ebd511d-c00e-42fc-b6fb-941af1d1cd80"], "metadata": {"type": "file", "idx": 2, "file": "two_layer_nn.py"}}}, "docstore/data": {"dcc1e679-ff3c-4681-8bd8-a4792660a0d9": {"__data__": {"id_": "dcc1e679-ff3c-4681-8bd8-a4792660a0d9", "embedding": null, "metadata": {"type": "file", "idx": 0, "file": "softmax_regression.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::0", "node_type": "4", "metadata": {"type": "file", "idx": 0, "file": "softmax_regression.py"}, "hash": "90ef7986db952af8fc2516921e592c1525d75139a61a6b748bf513489010d97e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "SoftmaxRegression module implementing a single-layer softmax classifier using a ReLU activation before softmax. The file defines SoftmaxRegression, a subclass of the project's base network interface (_baseNetwork), and manages weight initialization, forward passes, loss/accuracy computation, and gradient storage for training. It relies on numpy for numerical operations and uses several helper methods expected from the base class (e.g., ReLU, softmax, cross_entropy_loss, compute_accuracy, ReLU_dev). The forward method supports both training (computes and stores gradients) and evaluation (returns loss and accuracy without updating gradients). The network uses a weight matrix W1 without a bias term and updates gradients via a standard linear-backpropagation step preceded by a ReLU nonlinearity.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 802, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a62e25b-1e6e-4c7a-8fce-1c92ea5347fe": {"__data__": {"id_": "2a62e25b-1e6e-4c7a-8fce-1c92ea5347fe", "embedding": null, "metadata": {"type": "file", "idx": 1, "file": "_base_network.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::1", "node_type": "4", "metadata": {"type": "file", "idx": 1, "file": "_base_network.py"}, "hash": "34836c22706b80c6a1037482a2ec15fa3b982751cf517c167080eb6c304bf977", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This file defines a foundational neural network base class _baseNetwork that provides common utilities for simple feedforward models. It imports numpy and uses two dictionaries, weights and gradients, to manage parameters during training. The class implements numerically stable softmax, cross-entropy loss, and accuracy calculations, along with basic activations (sigmoid and ReLU) and their derivatives. There are placeholder methods for weight initialization (_weight_init) and the forward pass (forward) intended to be overridden by subclasses. Overall, it supplies essential building blocks for NumPy-based neural networks and is designed to be extended by more specific model implementations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 698, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1ebd511d-c00e-42fc-b6fb-941af1d1cd80": {"__data__": {"id_": "1ebd511d-c00e-42fc-b6fb-941af1d1cd80", "embedding": null, "metadata": {"type": "file", "idx": 2, "file": "two_layer_nn.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::2", "node_type": "4", "metadata": {"type": "file", "idx": 2, "file": "two_layer_nn.py"}, "hash": "709c1239821e2d515fccc767047984fbc776bd82d0b5ae26d6495da1fb072ea8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The file defines a simple two-layer neural network intended for multiclass classification, built on top of a common base network. It relies on the base class _baseNetwork for shared utilities such as activation functions, loss calculations, and metrics, and it maintains its parameters and gradients in dictionaries (self.weights and self.gradients). Weight initialization uses small random values for W1 and W2 with zero biases, and gradients are initialized to zeros. The forward method implements a full forward pass with a sigmoid hidden activation, a softmax output, cross-entropy loss, and accuracy computation, followed by a backpropagation step that fills self.gradients for W1, b1, W2, and b2. The implementation assumes inputs X of shape (N, input_size) and labels y of length N, and it interacts with the base network\u2019s methods for activation, loss, and gradient calculations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 887, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}