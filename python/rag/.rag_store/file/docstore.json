{"docstore/metadata": {"file::0": {"doc_hash": "9880b910c7d68a5028ed5853802a328e8a794a8a1060109bcd62890b383e4753"}, "file::1": {"doc_hash": "17543bc793276c93a9ae506758edec1ea673e26d003553e7e1d3d2e1cd96c0f0"}, "file::2": {"doc_hash": "593a4f6d0e1e1cdc4f13463773378d833829800b3ea5eac84de5c9998da5cd43"}, "1a01d364-f0eb-4d7b-8add-f3eab9c17c12": {"doc_hash": "790bb29d7cecf55870f9b19d6a76fed73869e690d8c58c4c04e4ed9b526fad39", "ref_doc_id": "file::0"}, "96faa4af-1129-46db-a603-30dcc237a4e7": {"doc_hash": "c05948fcc0fae7e39af488807f3f1109329e634e5b1391fdc8b3d51a363d4ed7", "ref_doc_id": "file::1"}, "ab2b0bce-d536-40c3-832d-6924310eb0d1": {"doc_hash": "e217fa203260610a143421edfec2bc28bcd642b0115cb3fc163868a545d74680", "ref_doc_id": "file::2"}}, "docstore/ref_doc_info": {"file::0": {"node_ids": ["1a01d364-f0eb-4d7b-8add-f3eab9c17c12"], "metadata": {"type": "file", "idx": 0, "file": "softmax_regression.py"}}, "file::1": {"node_ids": ["96faa4af-1129-46db-a603-30dcc237a4e7"], "metadata": {"type": "file", "idx": 1, "file": "_base_network.py"}}, "file::2": {"node_ids": ["ab2b0bce-d536-40c3-832d-6924310eb0d1"], "metadata": {"type": "file", "idx": 2, "file": "two_layer_nn.py"}}}, "docstore/data": {"1a01d364-f0eb-4d7b-8add-f3eab9c17c12": {"__data__": {"id_": "1a01d364-f0eb-4d7b-8add-f3eab9c17c12", "embedding": null, "metadata": {"type": "file", "idx": 0, "file": "softmax_regression.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::0", "node_type": "4", "metadata": {"type": "file", "idx": 0, "file": "softmax_regression.py"}, "hash": "9880b910c7d68a5028ed5853802a328e8a794a8a1060109bcd62890b383e4753", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Softmax regression single-layer model implemented in Python using numpy. The module defines a SoftmaxRegression class that inherits from a base network to perform a simple softmax classification with a single linear layer (no bias) followed by a ReLU activation and a softmax output. It initializes weights and gradients, computes forward passes including loss and accuracy, and, during training, performs backpropagation to populate gradients for the weight matrix W1. The implementation relies on utilities from the base network (activation, softmax, loss, and accuracy helpers) and stores parameters and gradients in dictionaries. External dependencies are numpy and the local base class module _base_network.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 712, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "96faa4af-1129-46db-a603-30dcc237a4e7": {"__data__": {"id_": "96faa4af-1129-46db-a603-30dcc237a4e7", "embedding": null, "metadata": {"type": "file", "idx": 1, "file": "_base_network.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::1", "node_type": "4", "metadata": {"type": "file", "idx": 1, "file": "_base_network.py"}, "hash": "17543bc793276c93a9ae506758edec1ea673e26d003553e7e1d3d2e1cd96c0f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The _base_network.py file defines a lightweight base class for neural networks using NumPy. It stores input_size and num_classes and maintains dictionaries for weights and gradients to be used by subclasses. It provides common utility implementations for activation (sigmoid, ReLU) and their derivatives, a numerically stable softmax, cross-entropy loss, and accuracy computation. The forward and weight initialization methods are left as placeholders to be implemented by concrete models. The module depends solely on NumPy and is designed to be extended by other network architectures within standard Python environments.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 623, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ab2b0bce-d536-40c3-832d-6924310eb0d1": {"__data__": {"id_": "ab2b0bce-d536-40c3-832d-6924310eb0d1", "embedding": null, "metadata": {"type": "file", "idx": 2, "file": "two_layer_nn.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "file::2", "node_type": "4", "metadata": {"type": "file", "idx": 2, "file": "two_layer_nn.py"}, "hash": "593a4f6d0e1e1cdc4f13463773378d833829800b3ea5eac84de5c9998da5cd43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "TwoLayerNet is a simple two-layer neural network implementation that extends a generic base network. It uses a hidden layer with a sigmoid activation and a softmax output layer to compute class probabilities, followed by cross-entropy loss and accuracy evaluation. The file handles parameter initialization, forward computation, and backpropagated gradients, storing parameters and gradients in dictionaries inherited from the base class. It depends on a common base class (_baseNetwork) for utility functions like activation, loss, and metrics, and it relies on numpy for numerical operations. The forward method currently performs both forward evaluation and gradient computation, with weight initialization designed for reproducible results via fixed seeds.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 760, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}