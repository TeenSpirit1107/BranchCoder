{
  "files": [
    {
      "file": "softmax_regression.py",
      "description": "Softmax regression single-layer model implemented in Python using numpy. The module defines a SoftmaxRegression class that inherits from a base network to perform a simple softmax classification with a single linear layer (no bias) followed by a ReLU activation and a softmax output. It initializes weights and gradients, computes forward passes including loss and accuracy, and, during training, performs backpropagation to populate gradients for the weight matrix W1. The implementation relies on utilities from the base network (activation, softmax, loss, and accuracy helpers) and stores parameters and gradients in dictionaries. External dependencies are numpy and the local base class module _base_network."
    },
    {
      "file": "_base_network.py",
      "description": "The _base_network.py file defines a lightweight base class for neural networks using NumPy. It stores input_size and num_classes and maintains dictionaries for weights and gradients to be used by subclasses. It provides common utility implementations for activation (sigmoid, ReLU) and their derivatives, a numerically stable softmax, cross-entropy loss, and accuracy computation. The forward and weight initialization methods are left as placeholders to be implemented by concrete models. The module depends solely on NumPy and is designed to be extended by other network architectures within standard Python environments."
    },
    {
      "file": "two_layer_nn.py",
      "description": "TwoLayerNet is a simple two-layer neural network implementation that extends a generic base network. It uses a hidden layer with a sigmoid activation and a softmax output layer to compute class probabilities, followed by cross-entropy loss and accuracy evaluation. The file handles parameter initialization, forward computation, and backpropagated gradients, storing parameters and gradients in dictionaries inherited from the base class. It depends on a common base class (_baseNetwork) for utility functions like activation, loss, and metrics, and it relies on numpy for numerical operations. The forward method currently performs both forward evaluation and gradient computation, with weight initialization designed for reproducible results via fixed seeds."
    }
  ],
  "functions": [
    {
      "file": "softmax_regression.py",
      "qualname": "softmax_regression.SoftmaxRegression.__init__",
      "source": "    def __init__(self, input_size=28*28, num_classes=10):\n        '''\n        A single layer softmax regression. The network is composed by:\n        a linear layer without bias => (optional ReLU activation) => Softmax\n        :param input_size: the input dimension\n        :param num_classes: the number of classes in total\n        '''\n        super().__init__(input_size, num_classes)\n        self._weight_init()\n",
      "calls": [
        "_base_network._baseNetwork.ReLU",
        "_base_network._baseNetwork.ReLU_dev",
        "_base_network._baseNetwork.__init__",
        "_base_network._baseNetwork._weight_init",
        "_base_network._baseNetwork.compute_accuracy",
        "_base_network._baseNetwork.cross_entropy_loss",
        "_base_network._baseNetwork.softmax",
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init"
      ],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Constructor that sets default input/class sizes, initializes the base class, and prepares weights via _weight_init."
    },
    {
      "file": "softmax_regression.py",
      "qualname": "softmax_regression.SoftmaxRegression._weight_init",
      "source": "    def _weight_init(self):\n        '''\n        initialize weights of the single layer regression network. No bias term included.\n        :return: None; self.weights is filled based on method\n        - W1: The weight matrix of the linear layer of shape (num_features, hidden_size)\n        '''\n        np.random.seed(1024)\n        self.weights['W1'] = 0.001 * np.random.randn(self.input_size, self.num_classes)\n        self.gradients['W1'] = np.zeros((self.input_size, self.num_classes))\n",
      "calls": [
        "_base_network._baseNetwork.ReLU",
        "_base_network._baseNetwork.ReLU_dev",
        "_base_network._baseNetwork.__init__",
        "_base_network._baseNetwork._weight_init",
        "_base_network._baseNetwork.compute_accuracy",
        "_base_network._baseNetwork.cross_entropy_loss",
        "_base_network._baseNetwork.softmax",
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init"
      ],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Initializes W1 with small random values (seeded for reproducibility) and allocates zeroed gradients for W1."
    },
    {
      "file": "softmax_regression.py",
      "qualname": "softmax_regression.SoftmaxRegression.forward",
      "source": "    def forward(self, X, y, mode='train'):\n        '''\n        Compute loss and gradients using softmax with vectorization.\n\n        :param X: a batch of image (N, 28x28)\n        :param y: labels of images in the batch (N,)\n        :return:\n            loss: the loss associated with the batch\n            accuracy: the accuracy of the batch\n        '''\n        loss = None\n        gradient = None\n        accuracy = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the forward process and compute the Cross-Entropy loss    #\n        #    2) Compute the gradient with respect to the loss                       #\n        # Hint:                                                                     #\n        #   Store your intermediate outputs before ReLU for backwards               #\n        #############################################################################\n        W1 = self.weights['W1']\n        Z = X.dot(W1)\n        A = self.ReLU(Z)\n        probs = self.softmax(A)\n        loss = self.cross_entropy_loss(probs, y)\n        accuracy = self.compute_accuracy(probs, y)\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        if mode != 'train':\n            return loss, accuracy\n\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the backward process:                                     #\n        #        1) Compute gradients of each weight by chain rule                  #\n        #        2) Store the gradients in self.gradients                           #\n        #############################################################################\n        N = X.shape[0]\n        dA = probs.copy()\n        dA[np.arange(N), y] -= 1.0\n        dA /= N\n\n        dZ = dA * self.ReLU_dev(Z)\n        dW1 = X.T.dot(dZ)\n        self.gradients['W1'] = dW1\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return loss, accuracy\n",
      "calls": [
        "_base_network._baseNetwork.ReLU",
        "_base_network._baseNetwork.ReLU_dev",
        "_base_network._baseNetwork.__init__",
        "_base_network._baseNetwork._weight_init",
        "_base_network._baseNetwork.compute_accuracy",
        "_base_network._baseNetwork.cross_entropy_loss",
        "_base_network._baseNetwork.softmax",
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init"
      ],
      "called_by": [],
      "description": "Performs the forward pass to compute loss and accuracy; in training mode, also computes and stores the gradient for W1 via backpropagation through ReLU."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.__init__",
      "source": "    def __init__(self, input_size=28 * 28, num_classes=10):\n\n        self.input_size = input_size\n        self.num_classes = num_classes\n\n        self.weights = dict()\n        self.gradients = dict()\n",
      "calls": [],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Initialize the base network with input_size and num_classes, and create empty dictionaries for weights and gradients."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork._weight_init",
      "source": "    def _weight_init(self):\n        pass\n",
      "calls": [],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Placeholder for weight initialization; currently no operation."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.forward",
      "source": "    def forward(self):\n        pass\n",
      "calls": [],
      "called_by": [],
      "description": "Placeholder for forward pass; intended to be implemented by subclasses."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.softmax",
      "source": "    def softmax(self, scores):\n        '''\n        Compute softmax scores given the raw output from the model\n\n        :param scores: raw scores from the model (N, num_classes)\n        :return:\n            prob: softmax probabilities (N, num_classes)\n        '''\n        prob = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Calculate softmax scores of input images                            #\n        #############################################################################\n        # numerically stable softmax\n        shifted = scores - np.max(scores, axis=1, keepdims=True)\n        exp_scores = np.exp(shifted)\n        sums = np.sum(exp_scores, axis=1, keepdims=True)\n        prob = exp_scores / sums\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return prob\n",
      "calls": [],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Compute numerically stable softmax probabilities for input scores of shape (N, num_classes); returns probabilities of the same shape."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.cross_entropy_loss",
      "source": "    def cross_entropy_loss(self, x_pred, y):\n        '''\n        Compute Cross-Entropy Loss based on prediction of the network and labels\n        :param x_pred: Probabilities from the model (N, num_classes)\n        :param y: Labels of instances in the batch\n        :return: The computed Cross-Entropy Loss\n        '''\n        loss = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement Cross-Entropy Loss                                        #\n        #############################################################################\n        # x_pred is probability distribution per row\n        N = x_pred.shape[0]\n        # avoid log(0)\n        eps = 1e-12\n        clipped = np.clip(x_pred, eps, 1.0)\n        correct_log_probs = -np.log(clipped[np.arange(N), y])\n        loss = np.mean(correct_log_probs)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return loss\n",
      "calls": [],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Compute mean cross-entropy loss from predicted probabilities and true labels; clips probabilities to avoid log(0) and returns a scalar loss."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.compute_accuracy",
      "source": "    def compute_accuracy(self, x_pred, y):\n        '''\n        Compute the accuracy of current batch\n        :param x_pred: Probabilities from the model (N, num_classes)\n        :param y: Labels of instances in the batch\n        :return: The accuracy of the batch\n        '''\n        acc = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the accuracy function                                     #\n        #############################################################################\n        preds = np.argmax(x_pred, axis=1)\n        acc = np.mean(preds == y)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return acc\n",
      "calls": [],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Compute accuracy by comparing predicted class with true labels and returning a scalar in [0, 1]."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.sigmoid",
      "source": "    def sigmoid(self, X):\n        '''\n        Compute the sigmoid activation for the input\n\n        :param X: the input data coming out from one layer of the model (N, layer size)\n        :return:\n            out: the value after the sigmoid activation is applied to the input (N, layer size)\n        '''\n        #############################################################################\n        # TODO: Comput the sigmoid activation on the input                          #\n        #############################################################################\n\n        out = 1.0 / (1.0 + np.exp(-X))\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return out\n",
      "calls": [],
      "called_by": [
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Compute the sigmoid activation for input X and return the activated values with the same shape."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.sigmoid_dev",
      "source": "    def sigmoid_dev(self, x):\n        '''\n        The analytical derivative of sigmoid function at x\n        :param x: Input data\n        :return: The derivative of sigmoid function at x\n        '''\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the derivative of Sigmoid function                        #\n        #############################################################################\n\n        s = 1.0 / (1.0 + np.exp(-x))\n        ds = s * (1.0 - s)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return ds\n",
      "calls": [],
      "called_by": [
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Compute the derivative of the sigmoid function with respect to its input."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.ReLU",
      "source": "    def ReLU(self, X):\n        '''\n        Compute the ReLU activation for the input\n\n        :param X: the input data coming out from one layer of the model (N, layer size)\n        :return:\n            out: the value after the ReLU activation is applied to the input (N, layer size)\n        '''\n        #############################################################################\n        # TODO: Comput the ReLU activation on the input                          #\n        #############################################################################\n\n        out = np.maximum(0, X)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return out\n",
      "calls": [],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward"
      ],
      "description": "Apply the ReLU activation elementwise to input X and return the result."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.ReLU_dev",
      "source": "    def ReLU_dev(self,X):\n        '''\n        Compute the gradient ReLU activation for the input\n\n        :param X: the input data coming out from one layer of the model (N, layer size)\n        :return:\n            out: gradient of ReLU given input X\n        '''\n        #############################################################################\n        # TODO: Comput the gradient of ReLU activation                              #\n        #############################################################################\n\n        out = (X > 0).astype(float)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return out\n",
      "calls": [],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward"
      ],
      "description": "Compute the gradient of ReLU given input X (1 for positive, 0 otherwise)."
    },
    {
      "file": "two_layer_nn.py",
      "qualname": "two_layer_nn.TwoLayerNet.__init__",
      "source": "    def __init__(self, input_size=28 * 28, num_classes=10, hidden_size=128):\n        super().__init__(input_size, num_classes)\n\n        self.hidden_size = hidden_size\n        self._weight_init()\n",
      "calls": [
        "_base_network._baseNetwork.__init__",
        "_base_network._baseNetwork._weight_init",
        "_base_network._baseNetwork.compute_accuracy",
        "_base_network._baseNetwork.cross_entropy_loss",
        "_base_network._baseNetwork.sigmoid",
        "_base_network._baseNetwork.sigmoid_dev",
        "_base_network._baseNetwork.softmax",
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init"
      ],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Initialize the network by delegating to the base class, set hidden layer size, and initialize weights and gradients via _weight_init."
    },
    {
      "file": "two_layer_nn.py",
      "qualname": "two_layer_nn.TwoLayerNet._weight_init",
      "source": "    def _weight_init(self):\n        '''\n        initialize weights of the network\n        :return: None; self.weights is filled based on method\n        - W1: The weight matrix of the first layer of shape (num_features, hidden_size)\n        - b1: The bias term of the first layer of shape (hidden_size,)\n        - W2: The weight matrix of the second layer of shape (hidden_size, num_classes)\n        - b2: The bias term of the second layer of shape (num_classes,)\n        '''\n\n        # initialize weights\n        self.weights['b1'] = np.zeros(self.hidden_size)\n        self.weights['b2'] = np.zeros(self.num_classes)\n        np.random.seed(1024)\n        self.weights['W1'] = 0.001 * np.random.randn(self.input_size, self.hidden_size)\n        np.random.seed(1024)\n        self.weights['W2'] = 0.001 * np.random.randn(self.hidden_size, self.num_classes)\n\n        # initialize gradients to zeros\n        self.gradients['W1'] = np.zeros((self.input_size, self.hidden_size))\n        self.gradients['b1'] = np.zeros(self.hidden_size)\n        self.gradients['W2'] = np.zeros((self.hidden_size, self.num_classes))\n        self.gradients['b2'] = np.zeros(self.num_classes)\n",
      "calls": [
        "_base_network._baseNetwork.__init__",
        "_base_network._baseNetwork._weight_init",
        "_base_network._baseNetwork.compute_accuracy",
        "_base_network._baseNetwork.cross_entropy_loss",
        "_base_network._baseNetwork.sigmoid",
        "_base_network._baseNetwork.sigmoid_dev",
        "_base_network._baseNetwork.softmax",
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init"
      ],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Initialize weights and biases (W1, b1, W2, b2) with small random values and zero gradients, using fixed seeds for reproducibility; shapes align with (input_size, hidden_size), (hidden_size,), (hidden_size, num_classes), and (num_classes,). This method populates self.weights and self.gradients and is called during construction."
    },
    {
      "file": "two_layer_nn.py",
      "qualname": "two_layer_nn.TwoLayerNet.forward",
      "source": "    def forward(self, X, y, mode='train'):\n        '''\n        The forward pass of the two-layer net. The activation function used in between the two layers is sigmoid, which\n        is to be implemented in self.,sigmoid.\n        The method forward should compute the loss of input batch X and gradients of each weights.\n        Further, it should also compute the accuracy of given batch. The loss and\n        accuracy are returned by the method and gradients are stored in self.gradients\n\n        :param X: a batch of images (N, input_size)\n        :param y: labels of images in the batch (N,)\n        :param mode: if mode is training, compute and update gradients;else, just return the loss and accuracy\n        :return:\n            loss: the loss associated with the batch\n            accuracy: the accuracy of the batch\n            self.gradients: gradients are not explicitly returned but rather updated in the class member self.gradients\n        '''\n        loss = None\n        accuracy = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the forward process:                                      #\n        #        1) Call sigmoid function between the two layers for non-linearity  #\n        #        2) The output of the second layer should be passed to softmax      #\n        #        function before computing the cross entropy loss                   #\n        #    2) Compute Cross-Entropy Loss and batch accuracy based on network      #\n        #       outputs                                                             #\n        #############################################################################\n\n        W1 = self.weights['W1']\n        b1 = self.weights['b1']\n        W2 = self.weights['W2']\n        b2 = self.weights['b2']\n\n        Z1 = X.dot(W1) + b1\n        A1 = self.sigmoid(Z1)\n        Z2 = A1.dot(W2) + b2\n        probs = self.softmax(Z2)\n        loss = self.cross_entropy_loss(probs, y)\n        accuracy = self.compute_accuracy(probs, y)\n\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the backward process:                                     #\n        #        1) Compute gradients of each weight and bias by chain rule         #\n        #        2) Store the gradients in self.gradients                           #\n        #    HINT: You will need to compute gradients backwards, i.e, compute       #\n        #          gradients of W2 and b2 first, then compute it for W1 and b1      #\n        #          You may also want to implement the analytical derivative of      #\n        #          the sigmoid function in self.sigmoid_dev first                   #\n        #############################################################################\n\n        N = X.shape[0]\n        dZ2 = probs.copy()\n        dZ2[np.arange(N), y] -= 1.0\n        dZ2 /= N\n\n        # Gradients for W2, b2\n        dW2 = A1.T.dot(dZ2)\n        db2 = np.sum(dZ2, axis=0)\n\n        # Backprop to hidden\n        dA1 = dZ2.dot(W2.T)\n        dZ1 = dA1 * self.sigmoid_dev(Z1)\n\n        # Gradients for W1, b1\n        dW1 = X.T.dot(dZ1)\n        db1 = np.sum(dZ1, axis=0)\n\n        self.gradients['W2'] = dW2\n        self.gradients['b2'] = db2\n        self.gradients['W1'] = dW1\n        self.gradients['b1'] = db1\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return loss, accuracy\n",
      "calls": [
        "_base_network._baseNetwork.__init__",
        "_base_network._baseNetwork._weight_init",
        "_base_network._baseNetwork.compute_accuracy",
        "_base_network._baseNetwork.cross_entropy_loss",
        "_base_network._baseNetwork.sigmoid",
        "_base_network._baseNetwork.sigmoid_dev",
        "_base_network._baseNetwork.softmax",
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init"
      ],
      "called_by": [],
      "description": "Perform a forward pass to compute the hidden activations (A1) with a sigmoid, the output scores (Z2) with a linear transform, and class probabilities via softmax; compute cross-entropy loss and accuracy; execute backpropagation to produce gradients for W2, b2, W1, and b1 and store them in self.gradients; return loss and accuracy. Note: the current implementation computes gradients regardless of the mode argument, and relies on base-class utilities for activation, loss, and metrics."
    }
  ],
  "classes": [
    {
      "file": "/home/yf/Workspace/deep_learning/assignment1/models/softmax_regression.py",
      "name": "SoftmaxRegression",
      "qualname": "<module>.SoftmaxRegression",
      "source": "class SoftmaxRegression(_baseNetwork):\n    def __init__(self, input_size=28*28, num_classes=10):\n        '''\n        A single layer softmax regression. The network is composed by:\n        a linear layer without bias => (optional ReLU activation) => Softmax\n        :param input_size: the input dimension\n        :param num_classes: the number of classes in total\n        '''\n        super().__init__(input_size, num_classes)\n        self._weight_init()\n\n    def _weight_init(self):\n        '''\n        initialize weights of the single layer regression network. No bias term included.\n        :return: None; self.weights is filled based on method\n        - W1: The weight matrix of the linear layer of shape (num_features, hidden_size)\n        '''\n        np.random.seed(1024)\n        self.weights['W1'] = 0.001 * np.random.randn(self.input_size, self.num_classes)\n        self.gradients['W1'] = np.zeros((self.input_size, self.num_classes))\n\n    def forward(self, X, y, mode='train'):\n        '''\n        Compute loss and gradients using softmax with vectorization.\n\n        :param X: a batch of image (N, 28x28)\n        :param y: labels of images in the batch (N,)\n        :return:\n            loss: the loss associated with the batch\n            accuracy: the accuracy of the batch\n        '''\n        loss = None\n        gradient = None\n        accuracy = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the forward process and compute the Cross-Entropy loss    #\n        #    2) Compute the gradient with respect to the loss                       #\n        # Hint:                                                                     #\n        #   Store your intermediate outputs before ReLU for backwards               #\n        #############################################################################\n        W1 = self.weights['W1']\n        Z = X.dot(W1)\n        A = self.ReLU(Z)\n        probs = self.softmax(A)\n        loss = self.cross_entropy_loss(probs, y)\n        accuracy = self.compute_accuracy(probs, y)\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        if mode != 'train':\n            return loss, accuracy\n\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the backward process:                                     #\n        #        1) Compute gradients of each weight by chain rule                  #\n        #        2) Store the gradients in self.gradients                           #\n        #############################################################################\n        N = X.shape[0]\n        dA = probs.copy()\n        dA[np.arange(N), y] -= 1.0\n        dA /= N\n\n        dZ = dA * self.ReLU_dev(Z)\n        dW1 = X.T.dot(dZ)\n        self.gradients['W1'] = dW1\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return loss, accuracy\n",
      "methods": [
        "<module>.SoftmaxRegression.__init__",
        "<module>.SoftmaxRegression._weight_init",
        "<module>.SoftmaxRegression.forward"
      ],
      "description": "A single-layer softmax regression model (no bias) that uses a linear layer outputs passed through ReLU and softmax. It manages weights and gradients, supports training/inference modes via forward, and relies on the base network for activation, loss, and accuracy utilities."
    },
    {
      "file": "/home/yf/Workspace/deep_learning/assignment1/models/_base_network.py",
      "name": "_baseNetwork",
      "qualname": "<module>._baseNetwork",
      "source": "class _baseNetwork:\n    def __init__(self, input_size=28 * 28, num_classes=10):\n\n        self.input_size = input_size\n        self.num_classes = num_classes\n\n        self.weights = dict()\n        self.gradients = dict()\n\n    def _weight_init(self):\n        pass\n\n    def forward(self):\n        pass\n\n    def softmax(self, scores):\n        '''\n        Compute softmax scores given the raw output from the model\n\n        :param scores: raw scores from the model (N, num_classes)\n        :return:\n            prob: softmax probabilities (N, num_classes)\n        '''\n        prob = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Calculate softmax scores of input images                            #\n        #############################################################################\n        # numerically stable softmax\n        shifted = scores - np.max(scores, axis=1, keepdims=True)\n        exp_scores = np.exp(shifted)\n        sums = np.sum(exp_scores, axis=1, keepdims=True)\n        prob = exp_scores / sums\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return prob\n\n    def cross_entropy_loss(self, x_pred, y):\n        '''\n        Compute Cross-Entropy Loss based on prediction of the network and labels\n        :param x_pred: Probabilities from the model (N, num_classes)\n        :param y: Labels of instances in the batch\n        :return: The computed Cross-Entropy Loss\n        '''\n        loss = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement Cross-Entropy Loss                                        #\n        #############################################################################\n        # x_pred is probability distribution per row\n        N = x_pred.shape[0]\n        # avoid log(0)\n        eps = 1e-12\n        clipped = np.clip(x_pred, eps, 1.0)\n        correct_log_probs = -np.log(clipped[np.arange(N), y])\n        loss = np.mean(correct_log_probs)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return loss\n\n    def compute_accuracy(self, x_pred, y):\n        '''\n        Compute the accuracy of current batch\n        :param x_pred: Probabilities from the model (N, num_classes)\n        :param y: Labels of instances in the batch\n        :return: The accuracy of the batch\n        '''\n        acc = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the accuracy function                                     #\n        #############################################################################\n        preds = np.argmax(x_pred, axis=1)\n        acc = np.mean(preds == y)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return acc\n\n    def sigmoid(self, X):\n        '''\n        Compute the sigmoid activation for the input\n\n        :param X: the input data coming out from one layer of the model (N, layer size)\n        :return:\n            out: the value after the sigmoid activation is applied to the input (N, layer size)\n        '''\n        #############################################################################\n        # TODO: Comput the sigmoid activation on the input                          #\n        #############################################################################\n\n        out = 1.0 / (1.0 + np.exp(-X))\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return out\n\n    def sigmoid_dev(self, x):\n        '''\n        The analytical derivative of sigmoid function at x\n        :param x: Input data\n        :return: The derivative of sigmoid function at x\n        '''\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the derivative of Sigmoid function                        #\n        #############################################################################\n\n        s = 1.0 / (1.0 + np.exp(-x))\n        ds = s * (1.0 - s)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return ds\n\n    def ReLU(self, X):\n        '''\n        Compute the ReLU activation for the input\n\n        :param X: the input data coming out from one layer of the model (N, layer size)\n        :return:\n            out: the value after the ReLU activation is applied to the input (N, layer size)\n        '''\n        #############################################################################\n        # TODO: Comput the ReLU activation on the input                          #\n        #############################################################################\n\n        out = np.maximum(0, X)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return out\n\n    def ReLU_dev(self,X):\n        '''\n        Compute the gradient ReLU activation for the input\n\n        :param X: the input data coming out from one layer of the model (N, layer size)\n        :return:\n            out: gradient of ReLU given input X\n        '''\n        #############################################################################\n        # TODO: Comput the gradient of ReLU activation                              #\n        #############################################################################\n\n        out = (X > 0).astype(float)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return out\n",
      "methods": [
        "<module>._baseNetwork.__init__",
        "<module>._baseNetwork._weight_init",
        "<module>._baseNetwork.forward",
        "<module>._baseNetwork.softmax",
        "<module>._baseNetwork.cross_entropy_loss",
        "<module>._baseNetwork.compute_accuracy",
        "<module>._baseNetwork.sigmoid",
        "<module>._baseNetwork.sigmoid_dev",
        "<module>._baseNetwork.ReLU",
        "<module>._baseNetwork.ReLU_dev"
      ],
      "description": "A minimal base class that stores configuration, holds placeholders for forward and weight initialization, and provides common neural-network utilities (softmax, cross-entropy loss, accuracy, sigmoid, and ReLU activations and derivatives) to be used or extended by concrete network implementations."
    },
    {
      "file": "/home/yf/Workspace/deep_learning/assignment1/models/two_layer_nn.py",
      "name": "TwoLayerNet",
      "qualname": "<module>.TwoLayerNet",
      "source": "class TwoLayerNet(_baseNetwork):\n    def __init__(self, input_size=28 * 28, num_classes=10, hidden_size=128):\n        super().__init__(input_size, num_classes)\n\n        self.hidden_size = hidden_size\n        self._weight_init()\n\n\n    def _weight_init(self):\n        '''\n        initialize weights of the network\n        :return: None; self.weights is filled based on method\n        - W1: The weight matrix of the first layer of shape (num_features, hidden_size)\n        - b1: The bias term of the first layer of shape (hidden_size,)\n        - W2: The weight matrix of the second layer of shape (hidden_size, num_classes)\n        - b2: The bias term of the second layer of shape (num_classes,)\n        '''\n\n        # initialize weights\n        self.weights['b1'] = np.zeros(self.hidden_size)\n        self.weights['b2'] = np.zeros(self.num_classes)\n        np.random.seed(1024)\n        self.weights['W1'] = 0.001 * np.random.randn(self.input_size, self.hidden_size)\n        np.random.seed(1024)\n        self.weights['W2'] = 0.001 * np.random.randn(self.hidden_size, self.num_classes)\n\n        # initialize gradients to zeros\n        self.gradients['W1'] = np.zeros((self.input_size, self.hidden_size))\n        self.gradients['b1'] = np.zeros(self.hidden_size)\n        self.gradients['W2'] = np.zeros((self.hidden_size, self.num_classes))\n        self.gradients['b2'] = np.zeros(self.num_classes)\n\n    def forward(self, X, y, mode='train'):\n        '''\n        The forward pass of the two-layer net. The activation function used in between the two layers is sigmoid, which\n        is to be implemented in self.,sigmoid.\n        The method forward should compute the loss of input batch X and gradients of each weights.\n        Further, it should also compute the accuracy of given batch. The loss and\n        accuracy are returned by the method and gradients are stored in self.gradients\n\n        :param X: a batch of images (N, input_size)\n        :param y: labels of images in the batch (N,)\n        :param mode: if mode is training, compute and update gradients;else, just return the loss and accuracy\n        :return:\n            loss: the loss associated with the batch\n            accuracy: the accuracy of the batch\n            self.gradients: gradients are not explicitly returned but rather updated in the class member self.gradients\n        '''\n        loss = None\n        accuracy = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the forward process:                                      #\n        #        1) Call sigmoid function between the two layers for non-linearity  #\n        #        2) The output of the second layer should be passed to softmax      #\n        #        function before computing the cross entropy loss                   #\n        #    2) Compute Cross-Entropy Loss and batch accuracy based on network      #\n        #       outputs                                                             #\n        #############################################################################\n\n        W1 = self.weights['W1']\n        b1 = self.weights['b1']\n        W2 = self.weights['W2']\n        b2 = self.weights['b2']\n\n        Z1 = X.dot(W1) + b1\n        A1 = self.sigmoid(Z1)\n        Z2 = A1.dot(W2) + b2\n        probs = self.softmax(Z2)\n        loss = self.cross_entropy_loss(probs, y)\n        accuracy = self.compute_accuracy(probs, y)\n\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the backward process:                                     #\n        #        1) Compute gradients of each weight and bias by chain rule         #\n        #        2) Store the gradients in self.gradients                           #\n        #    HINT: You will need to compute gradients backwards, i.e, compute       #\n        #          gradients of W2 and b2 first, then compute it for W1 and b1      #\n        #          You may also want to implement the analytical derivative of      #\n        #          the sigmoid function in self.sigmoid_dev first                   #\n        #############################################################################\n\n        N = X.shape[0]\n        dZ2 = probs.copy()\n        dZ2[np.arange(N), y] -= 1.0\n        dZ2 /= N\n\n        # Gradients for W2, b2\n        dW2 = A1.T.dot(dZ2)\n        db2 = np.sum(dZ2, axis=0)\n\n        # Backprop to hidden\n        dA1 = dZ2.dot(W2.T)\n        dZ1 = dA1 * self.sigmoid_dev(Z1)\n\n        # Gradients for W1, b1\n        dW1 = X.T.dot(dZ1)\n        db1 = np.sum(dZ1, axis=0)\n\n        self.gradients['W2'] = dW2\n        self.gradients['b2'] = db2\n        self.gradients['W1'] = dW1\n        self.gradients['b1'] = db1\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return loss, accuracy\n",
      "methods": [
        "<module>.TwoLayerNet.__init__",
        "<module>.TwoLayerNet._weight_init",
        "<module>.TwoLayerNet.forward"
      ],
      "description": "A concrete two-layer neural network with one hidden layer that uses sigmoid non-linearity in the hidden layer and a softmax output with cross-entropy loss; it manages parameter storage in self.weights and gradient storage in self.gradients and delegates common computational utilities to the _baseNetwork base class."
    }
  ]
}