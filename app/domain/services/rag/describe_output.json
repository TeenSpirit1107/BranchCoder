{
  "files": [
    {
      "file": "softmax_regression.py",
      "description": "SoftmaxRegression implements a single-layer softmax classifier without a bias term. It performs a linear projection from input features to class scores, applies an optional ReLU activation, and then applies softmax to produce class probabilities. The forward method computes cross-entropy loss and accuracy, and during training it also computes gradients with respect to the weight matrix W1 and stores them in a gradients dictionary. The implementation relies on numpy and on a base class _baseNetwork for common utilities like ReLU, softmax, loss, and accuracy computations. It stores parameters and gradients in dictionaries (self.weights and self.gradients) and exposes a small API via __init__, _weight_init, and forward."
    },
    {
      "file": "_base_network.py",
      "description": "[File-level summary] This module defines a foundational neural network class, _baseNetwork, intended for classification tasks (default input_size 28*28 and num_classes 10). It maintains dictionaries for weights and gradients to support parameterized models, and provides several utility methods for activations, loss computation, and accuracy assessment. The file includes implementations for softmax, cross-entropy loss, and common activations (sigmoid and ReLU) along with their derivatives. The actual weight initialization and forward-pass logic are left as placeholders to be implemented by subclasses. It relies solely on numpy and is designed to be extended by concrete network architectures."
    },
    {
      "file": "two_layer_nn.py",
      "description": "TwoLayerNet in two_layer_nn.py defines a compact 2-layer neural network with one hidden layer. It extends a base network to reuse utilities for activations, loss, and metrics, and to manage shared data structures like weights and gradients. The class initializes small random weights and zero biases, along with zero-initialized gradient buffers, via a dedicated weight initialization method. The forward method performs a two-stage forward pass (sigmoid activation between layers, softmax at the output), computes cross-entropy loss and accuracy, and computes/stores gradients for all parameters in the object. It relies on base-class utilities (sigmoid, sigmoid_dev, softmax, cross_entropy_loss, compute_accuracy) and numpy for numerical operations, and uses fixed seeds for reproducibility."
    }
  ],
  "functions": [
    {
      "file": "softmax_regression.py",
      "qualname": "softmax_regression.SoftmaxRegression.__init__",
      "source": "    def __init__(self, input_size=28*28, num_classes=10):\n        '''\n        A single layer softmax regression. The network is composed by:\n        a linear layer without bias => (optional ReLU activation) => Softmax\n        :param input_size: the input dimension\n        :param num_classes: the number of classes in total\n        '''\n        super().__init__(input_size, num_classes)\n        self._weight_init()\n",
      "calls": [
        "_base_network._baseNetwork.ReLU",
        "_base_network._baseNetwork.ReLU_dev",
        "_base_network._baseNetwork.__init__",
        "_base_network._baseNetwork._weight_init",
        "_base_network._baseNetwork.compute_accuracy",
        "_base_network._baseNetwork.cross_entropy_loss",
        "_base_network._baseNetwork.softmax",
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init"
      ],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Initialize the model with a given input size and number of classes, call the base class constructor, and perform weight initialization."
    },
    {
      "file": "softmax_regression.py",
      "qualname": "softmax_regression.SoftmaxRegression._weight_init",
      "source": "    def _weight_init(self):\n        '''\n        initialize weights of the single layer regression network. No bias term included.\n        :return: None; self.weights is filled based on method\n        - W1: The weight matrix of the linear layer of shape (num_features, hidden_size)\n        '''\n        np.random.seed(1024)\n        self.weights['W1'] = 0.001 * np.random.randn(self.input_size, self.num_classes)\n        self.gradients['W1'] = np.zeros((self.input_size, self.num_classes))\n",
      "calls": [
        "_base_network._baseNetwork.ReLU",
        "_base_network._baseNetwork.ReLU_dev",
        "_base_network._baseNetwork.__init__",
        "_base_network._baseNetwork._weight_init",
        "_base_network._baseNetwork.compute_accuracy",
        "_base_network._baseNetwork.cross_entropy_loss",
        "_base_network._baseNetwork.softmax",
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init"
      ],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Initialize the weight matrix W1 with small random values (seeded for reproducibility) and initialize the corresponding gradient storage to zeros."
    },
    {
      "file": "softmax_regression.py",
      "qualname": "softmax_regression.SoftmaxRegression.forward",
      "source": "    def forward(self, X, y, mode='train'):\n        '''\n        Compute loss and gradients using softmax with vectorization.\n\n        :param X: a batch of image (N, 28x28)\n        :param y: labels of images in the batch (N,)\n        :return:\n            loss: the loss associated with the batch\n            accuracy: the accuracy of the batch\n        '''\n        loss = None\n        gradient = None\n        accuracy = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the forward process and compute the Cross-Entropy loss    #\n        #    2) Compute the gradient with respect to the loss                       #\n        # Hint:                                                                     #\n        #   Store your intermediate outputs before ReLU for backwards               #\n        #############################################################################\n        W1 = self.weights['W1']\n        Z = X.dot(W1)\n        A = self.ReLU(Z)\n        probs = self.softmax(A)\n        loss = self.cross_entropy_loss(probs, y)\n        accuracy = self.compute_accuracy(probs, y)\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        if mode != 'train':\n            return loss, accuracy\n\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the backward process:                                     #\n        #        1) Compute gradients of each weight by chain rule                  #\n        #        2) Store the gradients in self.gradients                           #\n        #############################################################################\n        N = X.shape[0]\n        dA = probs.copy()\n        dA[np.arange(N), y] -= 1.0\n        dA /= N\n\n        dZ = dA * self.ReLU_dev(Z)\n        dW1 = X.T.dot(dZ)\n        self.gradients['W1'] = dW1\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return loss, accuracy\n",
      "calls": [
        "_base_network._baseNetwork.ReLU",
        "_base_network._baseNetwork.ReLU_dev",
        "_base_network._baseNetwork.__init__",
        "_base_network._baseNetwork._weight_init",
        "_base_network._baseNetwork.compute_accuracy",
        "_base_network._baseNetwork.cross_entropy_loss",
        "_base_network._baseNetwork.softmax",
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init"
      ],
      "called_by": [],
      "description": "Compute the forward pass to obtain Z = XW1, A = ReLU(Z), and probs via softmax; calculate loss and accuracy. If mode is 'train', perform backprop to compute dW1 and store it in self.gradients; otherwise, return loss and accuracy without gradient computation."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.__init__",
      "source": "    def __init__(self, input_size=28 * 28, num_classes=10):\n\n        self.input_size = input_size\n        self.num_classes = num_classes\n\n        self.weights = dict()\n        self.gradients = dict()\n",
      "calls": [],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Initialize the base network with input and class sizes, and create containers for weights and gradients; sets up internal configuration and default values."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork._weight_init",
      "source": "    def _weight_init(self):\n        pass\n",
      "calls": [],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Placeholder for weight initialization logic to be provided by subclasses; currently no operation."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.forward",
      "source": "    def forward(self):\n        pass\n",
      "calls": [],
      "called_by": [],
      "description": "Placeholder for the forward-pass implementation to be provided by subclasses; currently no operation."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.softmax",
      "source": "    def softmax(self, scores):\n        '''\n        Compute softmax scores given the raw output from the model\n\n        :param scores: raw scores from the model (N, num_classes)\n        :return:\n            prob: softmax probabilities (N, num_classes)\n        '''\n        prob = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Calculate softmax scores of input images                            #\n        #############################################################################\n        # numerically stable softmax\n        shifted = scores - np.max(scores, axis=1, keepdims=True)\n        exp_scores = np.exp(shifted)\n        sums = np.sum(exp_scores, axis=1, keepdims=True)\n        prob = exp_scores / sums\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return prob\n",
      "calls": [],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Compute numerically stable softmax probabilities from raw scores; inputs are (N, num_classes) scores and outputs are (N, num_classes) probabilities."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.cross_entropy_loss",
      "source": "    def cross_entropy_loss(self, x_pred, y):\n        '''\n        Compute Cross-Entropy Loss based on prediction of the network and labels\n        :param x_pred: Probabilities from the model (N, num_classes)\n        :param y: Labels of instances in the batch\n        :return: The computed Cross-Entropy Loss\n        '''\n        loss = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement Cross-Entropy Loss                                        #\n        #############################################################################\n        # x_pred is probability distribution per row\n        N = x_pred.shape[0]\n        # avoid log(0)\n        eps = 1e-12\n        clipped = np.clip(x_pred, eps, 1.0)\n        correct_log_probs = -np.log(clipped[np.arange(N), y])\n        loss = np.mean(correct_log_probs)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return loss\n",
      "calls": [],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Compute the mean cross-entropy loss given predicted probabilities (N, num_classes) and integer labels y; includes clipping to avoid log(0)."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.compute_accuracy",
      "source": "    def compute_accuracy(self, x_pred, y):\n        '''\n        Compute the accuracy of current batch\n        :param x_pred: Probabilities from the model (N, num_classes)\n        :param y: Labels of instances in the batch\n        :return: The accuracy of the batch\n        '''\n        acc = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the accuracy function                                     #\n        #############################################################################\n        preds = np.argmax(x_pred, axis=1)\n        acc = np.mean(preds == y)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return acc\n",
      "calls": [],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Calculate accuracy by comparing the predicted class (argmax over classes) with true labels; returns a scalar accuracy."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.sigmoid",
      "source": "    def sigmoid(self, X):\n        '''\n        Compute the sigmoid activation for the input\n\n        :param X: the input data coming out from one layer of the model (N, layer size)\n        :return:\n            out: the value after the sigmoid activation is applied to the input (N, layer size)\n        '''\n        #############################################################################\n        # TODO: Comput the sigmoid activation on the input                          #\n        #############################################################################\n\n        out = 1.0 / (1.0 + np.exp(-X))\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return out\n",
      "calls": [],
      "called_by": [
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Apply the sigmoid activation element-wise to input X (N, D); returns the activated array of the same shape."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.sigmoid_dev",
      "source": "    def sigmoid_dev(self, x):\n        '''\n        The analytical derivative of sigmoid function at x\n        :param x: Input data\n        :return: The derivative of sigmoid function at x\n        '''\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the derivative of Sigmoid function                        #\n        #############################################################################\n\n        s = 1.0 / (1.0 + np.exp(-x))\n        ds = s * (1.0 - s)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return ds\n",
      "calls": [],
      "called_by": [
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Compute the derivative of the sigmoid function for input x; returns s*(1-s) where s is the sigmoid of x."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.ReLU",
      "source": "    def ReLU(self, X):\n        '''\n        Compute the ReLU activation for the input\n\n        :param X: the input data coming out from one layer of the model (N, layer size)\n        :return:\n            out: the value after the ReLU activation is applied to the input (N, layer size)\n        '''\n        #############################################################################\n        # TODO: Comput the ReLU activation on the input                          #\n        #############################################################################\n\n        out = np.maximum(0, X)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return out\n",
      "calls": [],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward"
      ],
      "description": "Apply the ReLU activation (max(0, x)) element-wise to input X; returns the activated array."
    },
    {
      "file": "_base_network.py",
      "qualname": "_base_network._baseNetwork.ReLU_dev",
      "source": "    def ReLU_dev(self,X):\n        '''\n        Compute the gradient ReLU activation for the input\n\n        :param X: the input data coming out from one layer of the model (N, layer size)\n        :return:\n            out: gradient of ReLU given input X\n        '''\n        #############################################################################\n        # TODO: Comput the gradient of ReLU activation                              #\n        #############################################################################\n\n        out = (X > 0).astype(float)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return out\n",
      "calls": [],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward"
      ],
      "description": "Compute the derivative of ReLU, returning 1 for positive input and 0 otherwise."
    },
    {
      "file": "two_layer_nn.py",
      "qualname": "two_layer_nn.TwoLayerNet.__init__",
      "source": "    def __init__(self, input_size=28 * 28, num_classes=10, hidden_size=128):\n        super().__init__(input_size, num_classes)\n\n        self.hidden_size = hidden_size\n        self._weight_init()\n",
      "calls": [
        "_base_network._baseNetwork.__init__",
        "_base_network._baseNetwork._weight_init",
        "_base_network._baseNetwork.compute_accuracy",
        "_base_network._baseNetwork.cross_entropy_loss",
        "_base_network._baseNetwork.sigmoid",
        "_base_network._baseNetwork.sigmoid_dev",
        "_base_network._baseNetwork.softmax",
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init"
      ],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Initialize the network by setting input, output, and hidden sizes, calling the base class constructor, storing the hidden size, and triggering weight initialization."
    },
    {
      "file": "two_layer_nn.py",
      "qualname": "two_layer_nn.TwoLayerNet._weight_init",
      "source": "    def _weight_init(self):\n        '''\n        initialize weights of the network\n        :return: None; self.weights is filled based on method\n        - W1: The weight matrix of the first layer of shape (num_features, hidden_size)\n        - b1: The bias term of the first layer of shape (hidden_size,)\n        - W2: The weight matrix of the second layer of shape (hidden_size, num_classes)\n        - b2: The bias term of the second layer of shape (num_classes,)\n        '''\n\n        # initialize weights\n        self.weights['b1'] = np.zeros(self.hidden_size)\n        self.weights['b2'] = np.zeros(self.num_classes)\n        np.random.seed(1024)\n        self.weights['W1'] = 0.001 * np.random.randn(self.input_size, self.hidden_size)\n        np.random.seed(1024)\n        self.weights['W2'] = 0.001 * np.random.randn(self.hidden_size, self.num_classes)\n\n        # initialize gradients to zeros\n        self.gradients['W1'] = np.zeros((self.input_size, self.hidden_size))\n        self.gradients['b1'] = np.zeros(self.hidden_size)\n        self.gradients['W2'] = np.zeros((self.hidden_size, self.num_classes))\n        self.gradients['b2'] = np.zeros(self.num_classes)\n",
      "calls": [
        "_base_network._baseNetwork.__init__",
        "_base_network._baseNetwork._weight_init",
        "_base_network._baseNetwork.compute_accuracy",
        "_base_network._baseNetwork.cross_entropy_loss",
        "_base_network._baseNetwork.sigmoid",
        "_base_network._baseNetwork.sigmoid_dev",
        "_base_network._baseNetwork.softmax",
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init"
      ],
      "called_by": [
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "softmax_regression.SoftmaxRegression.forward",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init",
        "two_layer_nn.TwoLayerNet.forward"
      ],
      "description": "Allocate and initialize all weights and biases (W1, b1, W2, b2) and their corresponding gradient buffers; use small random values for weights and zeros for biases and gradients; relies on class attributes for dimensions and seeds for reproducibility."
    },
    {
      "file": "two_layer_nn.py",
      "qualname": "two_layer_nn.TwoLayerNet.forward",
      "source": "    def forward(self, X, y, mode='train'):\n        '''\n        The forward pass of the two-layer net. The activation function used in between the two layers is sigmoid, which\n        is to be implemented in self.,sigmoid.\n        The method forward should compute the loss of input batch X and gradients of each weights.\n        Further, it should also compute the accuracy of given batch. The loss and\n        accuracy are returned by the method and gradients are stored in self.gradients\n\n        :param X: a batch of images (N, input_size)\n        :param y: labels of images in the batch (N,)\n        :param mode: if mode is training, compute and update gradients;else, just return the loss and accuracy\n        :return:\n            loss: the loss associated with the batch\n            accuracy: the accuracy of the batch\n            self.gradients: gradients are not explicitly returned but rather updated in the class member self.gradients\n        '''\n        loss = None\n        accuracy = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the forward process:                                      #\n        #        1) Call sigmoid function between the two layers for non-linearity  #\n        #        2) The output of the second layer should be passed to softmax      #\n        #        function before computing the cross entropy loss                   #\n        #    2) Compute Cross-Entropy Loss and batch accuracy based on network      #\n        #       outputs                                                             #\n        #############################################################################\n\n        W1 = self.weights['W1']\n        b1 = self.weights['b1']\n        W2 = self.weights['W2']\n        b2 = self.weights['b2']\n\n        Z1 = X.dot(W1) + b1\n        A1 = self.sigmoid(Z1)\n        Z2 = A1.dot(W2) + b2\n        probs = self.softmax(Z2)\n        loss = self.cross_entropy_loss(probs, y)\n        accuracy = self.compute_accuracy(probs, y)\n\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the backward process:                                     #\n        #        1) Compute gradients of each weight and bias by chain rule         #\n        #        2) Store the gradients in self.gradients                           #\n        #    HINT: You will need to compute gradients backwards, i.e, compute       #\n        #          gradients of W2 and b2 first, then compute it for W1 and b1      #\n        #          You may also want to implement the analytical derivative of      #\n        #          the sigmoid function in self.sigmoid_dev first                   #\n        #############################################################################\n\n        N = X.shape[0]\n        dZ2 = probs.copy()\n        dZ2[np.arange(N), y] -= 1.0\n        dZ2 /= N\n\n        # Gradients for W2, b2\n        dW2 = A1.T.dot(dZ2)\n        db2 = np.sum(dZ2, axis=0)\n\n        # Backprop to hidden\n        dA1 = dZ2.dot(W2.T)\n        dZ1 = dA1 * self.sigmoid_dev(Z1)\n\n        # Gradients for W1, b1\n        dW1 = X.T.dot(dZ1)\n        db1 = np.sum(dZ1, axis=0)\n\n        self.gradients['W2'] = dW2\n        self.gradients['b2'] = db2\n        self.gradients['W1'] = dW1\n        self.gradients['b1'] = db1\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return loss, accuracy\n",
      "calls": [
        "_base_network._baseNetwork.__init__",
        "_base_network._baseNetwork._weight_init",
        "_base_network._baseNetwork.compute_accuracy",
        "_base_network._baseNetwork.cross_entropy_loss",
        "_base_network._baseNetwork.sigmoid",
        "_base_network._baseNetwork.sigmoid_dev",
        "_base_network._baseNetwork.softmax",
        "softmax_regression.SoftmaxRegression.__init__",
        "softmax_regression.SoftmaxRegression._weight_init",
        "two_layer_nn.TwoLayerNet.__init__",
        "two_layer_nn.TwoLayerNet._weight_init"
      ],
      "called_by": [],
      "description": "Perform a forward pass through the two-layer network (X -> Z1 -> A1 -> Z2 -> probs), compute loss and accuracy, and calculate gradients via backpropagation, storing them in self.gradients for W1, b1, W2, and b2; return the loss and accuracy."
    }
  ],
  "classes": [
    {
      "file": "/home/yf/Workspace/deep_learning/assignment1/models/softmax_regression.py",
      "name": "SoftmaxRegression",
      "qualname": "<module>.SoftmaxRegression",
      "source": "class SoftmaxRegression(_baseNetwork):\n    def __init__(self, input_size=28*28, num_classes=10):\n        '''\n        A single layer softmax regression. The network is composed by:\n        a linear layer without bias => (optional ReLU activation) => Softmax\n        :param input_size: the input dimension\n        :param num_classes: the number of classes in total\n        '''\n        super().__init__(input_size, num_classes)\n        self._weight_init()\n\n    def _weight_init(self):\n        '''\n        initialize weights of the single layer regression network. No bias term included.\n        :return: None; self.weights is filled based on method\n        - W1: The weight matrix of the linear layer of shape (num_features, hidden_size)\n        '''\n        np.random.seed(1024)\n        self.weights['W1'] = 0.001 * np.random.randn(self.input_size, self.num_classes)\n        self.gradients['W1'] = np.zeros((self.input_size, self.num_classes))\n\n    def forward(self, X, y, mode='train'):\n        '''\n        Compute loss and gradients using softmax with vectorization.\n\n        :param X: a batch of image (N, 28x28)\n        :param y: labels of images in the batch (N,)\n        :return:\n            loss: the loss associated with the batch\n            accuracy: the accuracy of the batch\n        '''\n        loss = None\n        gradient = None\n        accuracy = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the forward process and compute the Cross-Entropy loss    #\n        #    2) Compute the gradient with respect to the loss                       #\n        # Hint:                                                                     #\n        #   Store your intermediate outputs before ReLU for backwards               #\n        #############################################################################\n        W1 = self.weights['W1']\n        Z = X.dot(W1)\n        A = self.ReLU(Z)\n        probs = self.softmax(A)\n        loss = self.cross_entropy_loss(probs, y)\n        accuracy = self.compute_accuracy(probs, y)\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        if mode != 'train':\n            return loss, accuracy\n\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the backward process:                                     #\n        #        1) Compute gradients of each weight by chain rule                  #\n        #        2) Store the gradients in self.gradients                           #\n        #############################################################################\n        N = X.shape[0]\n        dA = probs.copy()\n        dA[np.arange(N), y] -= 1.0\n        dA /= N\n\n        dZ = dA * self.ReLU_dev(Z)\n        dW1 = X.T.dot(dZ)\n        self.gradients['W1'] = dW1\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return loss, accuracy\n",
      "methods": [
        "<module>.SoftmaxRegression.__init__",
        "<module>.SoftmaxRegression._weight_init",
        "<module>.SoftmaxRegression.forward"
      ],
      "description": "A single-layer softmax regression model without bias that uses a linear projection followed by ReLU and softmax, manages parameters and gradients via dictionaries, and leverages base network utilities for activations and losses. It defines __init__, _weight_init, and forward to support training and evaluation."
    },
    {
      "file": "/home/yf/Workspace/deep_learning/assignment1/models/_base_network.py",
      "name": "_baseNetwork",
      "qualname": "<module>._baseNetwork",
      "source": "class _baseNetwork:\n    def __init__(self, input_size=28 * 28, num_classes=10):\n\n        self.input_size = input_size\n        self.num_classes = num_classes\n\n        self.weights = dict()\n        self.gradients = dict()\n\n    def _weight_init(self):\n        pass\n\n    def forward(self):\n        pass\n\n    def softmax(self, scores):\n        '''\n        Compute softmax scores given the raw output from the model\n\n        :param scores: raw scores from the model (N, num_classes)\n        :return:\n            prob: softmax probabilities (N, num_classes)\n        '''\n        prob = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Calculate softmax scores of input images                            #\n        #############################################################################\n        # numerically stable softmax\n        shifted = scores - np.max(scores, axis=1, keepdims=True)\n        exp_scores = np.exp(shifted)\n        sums = np.sum(exp_scores, axis=1, keepdims=True)\n        prob = exp_scores / sums\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return prob\n\n    def cross_entropy_loss(self, x_pred, y):\n        '''\n        Compute Cross-Entropy Loss based on prediction of the network and labels\n        :param x_pred: Probabilities from the model (N, num_classes)\n        :param y: Labels of instances in the batch\n        :return: The computed Cross-Entropy Loss\n        '''\n        loss = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement Cross-Entropy Loss                                        #\n        #############################################################################\n        # x_pred is probability distribution per row\n        N = x_pred.shape[0]\n        # avoid log(0)\n        eps = 1e-12\n        clipped = np.clip(x_pred, eps, 1.0)\n        correct_log_probs = -np.log(clipped[np.arange(N), y])\n        loss = np.mean(correct_log_probs)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return loss\n\n    def compute_accuracy(self, x_pred, y):\n        '''\n        Compute the accuracy of current batch\n        :param x_pred: Probabilities from the model (N, num_classes)\n        :param y: Labels of instances in the batch\n        :return: The accuracy of the batch\n        '''\n        acc = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the accuracy function                                     #\n        #############################################################################\n        preds = np.argmax(x_pred, axis=1)\n        acc = np.mean(preds == y)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return acc\n\n    def sigmoid(self, X):\n        '''\n        Compute the sigmoid activation for the input\n\n        :param X: the input data coming out from one layer of the model (N, layer size)\n        :return:\n            out: the value after the sigmoid activation is applied to the input (N, layer size)\n        '''\n        #############################################################################\n        # TODO: Comput the sigmoid activation on the input                          #\n        #############################################################################\n\n        out = 1.0 / (1.0 + np.exp(-X))\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return out\n\n    def sigmoid_dev(self, x):\n        '''\n        The analytical derivative of sigmoid function at x\n        :param x: Input data\n        :return: The derivative of sigmoid function at x\n        '''\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the derivative of Sigmoid function                        #\n        #############################################################################\n\n        s = 1.0 / (1.0 + np.exp(-x))\n        ds = s * (1.0 - s)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return ds\n\n    def ReLU(self, X):\n        '''\n        Compute the ReLU activation for the input\n\n        :param X: the input data coming out from one layer of the model (N, layer size)\n        :return:\n            out: the value after the ReLU activation is applied to the input (N, layer size)\n        '''\n        #############################################################################\n        # TODO: Comput the ReLU activation on the input                          #\n        #############################################################################\n\n        out = np.maximum(0, X)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return out\n\n    def ReLU_dev(self,X):\n        '''\n        Compute the gradient ReLU activation for the input\n\n        :param X: the input data coming out from one layer of the model (N, layer size)\n        :return:\n            out: gradient of ReLU given input X\n        '''\n        #############################################################################\n        # TODO: Comput the gradient of ReLU activation                              #\n        #############################################################################\n\n        out = (X > 0).astype(float)\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return out\n",
      "methods": [
        "<module>._baseNetwork.__init__",
        "<module>._baseNetwork._weight_init",
        "<module>._baseNetwork.forward",
        "<module>._baseNetwork.softmax",
        "<module>._baseNetwork.cross_entropy_loss",
        "<module>._baseNetwork.compute_accuracy",
        "<module>._baseNetwork.sigmoid",
        "<module>._baseNetwork.sigmoid_dev",
        "<module>._baseNetwork.ReLU",
        "<module>._baseNetwork.ReLU_dev"
      ],
      "description": "A foundational neural network class that encapsulates common utilities for classification networks, including handling of input size, number of classes, and dictionaries for weights/gradients; provides standard activations, loss, and accuracy helpers and defines placeholders for weight initialization and forward computation to be implemented by subclasses."
    },
    {
      "file": "/home/yf/Workspace/deep_learning/assignment1/models/two_layer_nn.py",
      "name": "TwoLayerNet",
      "qualname": "<module>.TwoLayerNet",
      "source": "class TwoLayerNet(_baseNetwork):\n    def __init__(self, input_size=28 * 28, num_classes=10, hidden_size=128):\n        super().__init__(input_size, num_classes)\n\n        self.hidden_size = hidden_size\n        self._weight_init()\n\n\n    def _weight_init(self):\n        '''\n        initialize weights of the network\n        :return: None; self.weights is filled based on method\n        - W1: The weight matrix of the first layer of shape (num_features, hidden_size)\n        - b1: The bias term of the first layer of shape (hidden_size,)\n        - W2: The weight matrix of the second layer of shape (hidden_size, num_classes)\n        - b2: The bias term of the second layer of shape (num_classes,)\n        '''\n\n        # initialize weights\n        self.weights['b1'] = np.zeros(self.hidden_size)\n        self.weights['b2'] = np.zeros(self.num_classes)\n        np.random.seed(1024)\n        self.weights['W1'] = 0.001 * np.random.randn(self.input_size, self.hidden_size)\n        np.random.seed(1024)\n        self.weights['W2'] = 0.001 * np.random.randn(self.hidden_size, self.num_classes)\n\n        # initialize gradients to zeros\n        self.gradients['W1'] = np.zeros((self.input_size, self.hidden_size))\n        self.gradients['b1'] = np.zeros(self.hidden_size)\n        self.gradients['W2'] = np.zeros((self.hidden_size, self.num_classes))\n        self.gradients['b2'] = np.zeros(self.num_classes)\n\n    def forward(self, X, y, mode='train'):\n        '''\n        The forward pass of the two-layer net. The activation function used in between the two layers is sigmoid, which\n        is to be implemented in self.,sigmoid.\n        The method forward should compute the loss of input batch X and gradients of each weights.\n        Further, it should also compute the accuracy of given batch. The loss and\n        accuracy are returned by the method and gradients are stored in self.gradients\n\n        :param X: a batch of images (N, input_size)\n        :param y: labels of images in the batch (N,)\n        :param mode: if mode is training, compute and update gradients;else, just return the loss and accuracy\n        :return:\n            loss: the loss associated with the batch\n            accuracy: the accuracy of the batch\n            self.gradients: gradients are not explicitly returned but rather updated in the class member self.gradients\n        '''\n        loss = None\n        accuracy = None\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the forward process:                                      #\n        #        1) Call sigmoid function between the two layers for non-linearity  #\n        #        2) The output of the second layer should be passed to softmax      #\n        #        function before computing the cross entropy loss                   #\n        #    2) Compute Cross-Entropy Loss and batch accuracy based on network      #\n        #       outputs                                                             #\n        #############################################################################\n\n        W1 = self.weights['W1']\n        b1 = self.weights['b1']\n        W2 = self.weights['W2']\n        b2 = self.weights['b2']\n\n        Z1 = X.dot(W1) + b1\n        A1 = self.sigmoid(Z1)\n        Z2 = A1.dot(W2) + b2\n        probs = self.softmax(Z2)\n        loss = self.cross_entropy_loss(probs, y)\n        accuracy = self.compute_accuracy(probs, y)\n\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n\n        #############################################################################\n        # TODO:                                                                     #\n        #    1) Implement the backward process:                                     #\n        #        1) Compute gradients of each weight and bias by chain rule         #\n        #        2) Store the gradients in self.gradients                           #\n        #    HINT: You will need to compute gradients backwards, i.e, compute       #\n        #          gradients of W2 and b2 first, then compute it for W1 and b1      #\n        #          You may also want to implement the analytical derivative of      #\n        #          the sigmoid function in self.sigmoid_dev first                   #\n        #############################################################################\n\n        N = X.shape[0]\n        dZ2 = probs.copy()\n        dZ2[np.arange(N), y] -= 1.0\n        dZ2 /= N\n\n        # Gradients for W2, b2\n        dW2 = A1.T.dot(dZ2)\n        db2 = np.sum(dZ2, axis=0)\n\n        # Backprop to hidden\n        dA1 = dZ2.dot(W2.T)\n        dZ1 = dA1 * self.sigmoid_dev(Z1)\n\n        # Gradients for W1, b1\n        dW1 = X.T.dot(dZ1)\n        db1 = np.sum(dZ1, axis=0)\n\n        self.gradients['W2'] = dW2\n        self.gradients['b2'] = db2\n        self.gradients['W1'] = dW1\n        self.gradients['b1'] = db1\n\n        #############################################################################\n        #                              END OF YOUR CODE                             #\n        #############################################################################\n        return loss, accuracy\n",
      "methods": [
        "<module>.TwoLayerNet.__init__",
        "<module>.TwoLayerNet._weight_init",
        "<module>.TwoLayerNet.forward"
      ],
      "description": "A two-layer neural network with one hidden layer that manages parameters (weights and biases) and their gradients, and uses base-network utilities to compute activations, loss, and metrics; core methods include initialization, weight setup, and a forward/backward pass to produce loss, accuracy, and parameter gradients."
    }
  ]
}